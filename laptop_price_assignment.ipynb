{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260c49c4",
   "metadata": {},
   "source": [
    "# Laptop Price Prediction â€” End-to-End\n",
    "\n",
    "**Author:** Kartik\n",
    "\n",
    "This notebook contains: EDA, Cleaning, Feature Engineering, Modeling (sklearn), evaluation, and saved artifacts.\n",
    "\n",
    "Run top-to-bottom. If a package is missing use `!pip install <package>` in a cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports (run once)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, re, math, json\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import joblib\n",
    "print('Libraries loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "RAW_CSV = '/mnt/data/laptop_price - dataset.csv'\n",
    "CLEANED_CSV = '/mnt/data/laptop_price.cleaned.csv'\n",
    "RESULTS_CSV = '/mnt/data/model_results.csv'\n",
    "PLOTS_DIR = '/mnt/data/plots'\n",
    "MODEL_OUT = '/mnt/data/best_model_pipeline.joblib'\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "print('Paths set.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60482eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read dataset\n",
    "def safe_read_csv(path):\n",
    "    for enc in ['utf-8','utf-8-sig','latin-1']:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.read_csv(path, engine='python')\n",
    "\n",
    "df = safe_read_csv(RAW_CSV)\n",
    "print('Shape:', df.shape)\n",
    "display(df.head(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58215661",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Inspect columns, missing values, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9368b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Columns:', df.columns.tolist())\n",
    "display(df.isna().sum().sort_values(ascending=False).head(20))\n",
    "display(df.describe(include='all').T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de71885",
   "metadata": {},
   "source": [
    "## Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper funcs\n",
    "import numpy as np, re\n",
    "def extract_resolution(x):\n",
    "    if pd.isna(x): return (np.nan, np.nan)\n",
    "    m = re.search(r'(\\d{3,4})\\s*[xX]\\s*(\\d{3,4})', str(x))\n",
    "    if m: return int(m.group(1)), int(m.group(2))\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "def parse_storage(mem):\n",
    "    d = {'SSD_GB':0,'HDD_GB':0,'Flash_GB':0,'Hybrid_GB':0,'SSHD_GB':0}\n",
    "    if pd.isna(mem): return d\n",
    "    parts = re.split(r'\\s*\\+\\s*', str(mem))\n",
    "    for p in parts:\n",
    "        pm = re.search(r'(\\d+)\\s*TB', p, flags=re.I)\n",
    "        if pm: size = int(pm.group(1))*1024\n",
    "        else:\n",
    "            pm2 = re.search(r'(\\d+)\\s*GB', p, flags=re.I)\n",
    "            size = int(pm2.group(1)) if pm2 else 0\n",
    "        if re.search(r'SSD', p, flags=re.I): d['SSD_GB'] += size\n",
    "        elif re.search(r'SSHD', p, flags=re.I): d['SSHD_GB'] += size\n",
    "        elif re.search(r'Hybrid', p, flags=re.I): d['Hybrid_GB'] += size\n",
    "        elif re.search(r'Flash', p, flags=re.I): d['Flash_GB'] += size\n",
    "        elif re.search(r'HDD', p, flags=re.I): d['HDD_GB'] += size\n",
    "        else: d['HDD_GB'] += size\n",
    "    return d\n",
    "\n",
    "def compute_ppi(w,h,inch):\n",
    "    try:\n",
    "        if np.any(pd.isna([w,h,inch])) or inch==0: return np.nan\n",
    "        return ((w**2 + h**2)**0.5)/inch\n",
    "    except: return np.nan\n",
    "\n",
    "# rename columns\n",
    "df.columns = [c.strip().replace('\\n',' ').replace('(GB)','GB').replace('(kg)','kg') for c in df.columns]\n",
    "rename_map = {}\n",
    "for c in df.columns:\n",
    "    if c.lower().strip() == 'ram (gb)': rename_map[c] = 'RAM_GB'\n",
    "    if c.lower().strip() == 'price (euro)': rename_map[c] = 'Price_Euro'\n",
    "    if c.lower().strip() == 'weight (kg)': rename_map[c] = 'Weight_kg'\n",
    "    if c.lower().strip().startswith('cpu_frequency'): rename_map[c] = 'CPU_Frequency'\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# resolution parsing\n",
    "if 'ScreenResolution' in df.columns:\n",
    "    df['ResWidth'], df['ResHeight'] = zip(*df['ScreenResolution'].map(extract_resolution))\n",
    "else:\n",
    "    df['ResWidth'] = df['ResHeight'] = np.nan\n",
    "\n",
    "# PPI\n",
    "df['PPI'] = [compute_ppi(w,h,inch) for w,h,inch in zip(df.get('ResWidth',[]), df.get('ResHeight',[]), df.get('Inches',[]))]\n",
    "\n",
    "# Memory parse\n",
    "if 'Memory' in df.columns:\n",
    "    mem_df = df['Memory'].map(parse_storage).apply(pd.Series)\n",
    "    df = pd.concat([df, mem_df], axis=1)\n",
    "\n",
    "# numeric parse helper\n",
    "def parse_first_number(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r'([\\d\\.]+)', str(x))\n",
    "    return float(m.group(1)) if m else np.nan\n",
    "\n",
    "for col in ['RAM_GB','Weight_kg','CPU_Frequency']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(parse_first_number)\n",
    "\n",
    "# clip outliers\n",
    "num_cols = [c for c in ['Inches','CPU_Frequency','RAM_GB','Weight_kg','Price_Euro','PPI','ResWidth','ResHeight',\n",
    "                        'SSD_GB','HDD_GB','Flash_GB','Hybrid_GB','SSHD_GB'] if c in df.columns]\n",
    "for c in num_cols:\n",
    "    lo, hi = df[c].quantile([0.01,0.99]).values\n",
    "    df[c] = df[c].clip(lo, hi)\n",
    "\n",
    "# save cleaned\n",
    "df.to_csv(CLEANED_CSV, index=False)\n",
    "print('Saved cleaned to', CLEANED_CSV)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf68313",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save simple histograms and heatmap\n",
    "numeric_cols = [c for c in df.select_dtypes(include=[np.number]).columns if df[c].nunique()>1]\n",
    "\n",
    "def save_hist(col):\n",
    "    try:\n",
    "        ax = df[col].dropna().plot(kind='hist', bins=30, title=f'Distribution: {col}')\n",
    "        fig = ax.get_figure()\n",
    "        path = os.path.join(PLOTS_DIR, f'hist_{col}.png')\n",
    "        fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "plot_paths = []\n",
    "for c in ['Price_Euro'] + [x for x in numeric_cols if x!='Price_Euro'][:6]:\n",
    "    p = save_hist(c)\n",
    "    if p: plot_paths.append(p)\n",
    "\n",
    "try:\n",
    "    corr = df[numeric_cols].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(corr, aspect='auto')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr)), corr.columns)\n",
    "    path = os.path.join(PLOTS_DIR,'corr_heatmap.png')\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    plot_paths.append(path)\n",
    "except Exception as e:\n",
    "    print('Heatmap failed:', e)\n",
    "\n",
    "print('Saved plots:', plot_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b785ea5",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modeling pipeline and training\n",
    "price_cols = [c for c in df.columns if re.search(r'price', c, flags=re.I)]\n",
    "assert len(price_cols)>=1, 'No price column found.'\n",
    "TARGET = price_cols[0]\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print('num:', num_features)\n",
    "print('cat sample:', cat_features[:8])\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "preproc = ColumnTransformer([('num', num_pipeline, num_features), ('cat', cat_pipeline, cat_features)], remainder='drop')\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': (LinearRegression(), {}),\n",
    "    'DecisionTree': (DecisionTreeRegressor(random_state=42), {'model__max_depth':[4,8,12,None]}),\n",
    "    'RandomForest': (RandomForestRegressor(random_state=42, n_jobs=-1), {'model__n_estimators':[100], 'model__max_depth':[None,12]}),\n",
    "    'GradientBoosting': (GradientBoostingRegressor(random_state=42), {'model__n_estimators':[100], 'model__max_depth':[2,3], 'model__learning_rate':[0.05,0.1]})\n",
    "}\n",
    "\n",
    "results=[]; best_models={}\n",
    "for name,(est,grid) in models.items():\n",
    "    print('\\nTraining', name)\n",
    "    pipe = Pipeline([('preprocess', preproc), ('model', est)])\n",
    "    if grid:\n",
    "        gs = GridSearchCV(pipe, param_grid=grid, cv=4, scoring='r2', n_jobs=-1, verbose=0)\n",
    "        gs.fit(X_train, y_train)\n",
    "        best = gs.best_estimator_; cv_score = gs.best_score_; best_params = gs.best_params_\n",
    "    else:\n",
    "        pipe.fit(X_train, y_train); best = pipe; cv_score=None; best_params={}\n",
    "    y_pred = best.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred); mse = mean_squared_error(y_test, y_pred); rmse = math.sqrt(mse)\n",
    "    results.append({'Model':name,'CV_R2':cv_score,'Test_R2':r2,'Test_RMSE':rmse,'Best_Params':best_params})\n",
    "    best_models[name] = best\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Test_R2',ascending=False).reset_index(drop=True)\n",
    "display(results_df)\n",
    "results_df.to_csv(RESULTS_CSV, index=False)\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "best_pipeline = best_models[best_name]\n",
    "joblib.dump(best_pipeline, MODEL_OUT)\n",
    "print('Saved model:', MODEL_OUT, ' Best:', best_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b025bd5",
   "metadata": {},
   "source": [
    "## Feature importances (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    pre = best_pipeline.named_steps['preprocess']\n",
    "    ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(ohe.get_feature_names_out(cat_features)) if len(cat_features)>0 else []\n",
    "except Exception:\n",
    "    cat_names = []\n",
    "\n",
    "all_names = num_features + cat_names\n",
    "importance = None\n",
    "try:\n",
    "    model_inner = best_pipeline.named_steps['model']\n",
    "    if hasattr(model_inner, 'feature_importances_'):\n",
    "        importance = model_inner.feature_importances_\n",
    "    elif hasattr(model_inner, 'coef_'):\n",
    "        importance = np.abs(model_inner.coef_).ravel()\n",
    "except Exception as e:\n",
    "    print('Importance extraction failed:', e)\n",
    "\n",
    "if importance is not None and len(importance)==len(all_names):\n",
    "    imp_df = pd.DataFrame({'Feature':all_names,'Importance':importance}).sort_values('Importance',ascending=False).head(25)\n",
    "    display(imp_df)\n",
    "    path = os.path.join(PLOTS_DIR, f'feature_importances_{best_name}.png')\n",
    "    plt.figure(figsize=(8,6)); plt.barh(imp_df['Feature'], imp_df['Importance']); plt.gca().invert_yaxis(); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "    print('Saved importances to', path)\n",
    "else:\n",
    "    print('No importances available or length mismatch.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9f58c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The notebook saved cleaned data, model pipeline, and results. You can further tune models or export as PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Done. Files created:')\n",
    "print('- Cleaned CSV:', CLEANED_CSV)\n",
    "print('- Results CSV:', RESULTS_CSV)\n",
    "print('- Model pipeline:', MODEL_OUT)\n",
    "print('- Plots dir:', PLOTS_DIR)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
